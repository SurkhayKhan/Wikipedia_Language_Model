# -*- coding: utf-8 -*-
"""with_back_off.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xMsdQ6LD45ZwZ043gip3kAcIYeflPTtZ
"""

import re
from collections import defaultdict, Counter
from math import log2

def preprocess(text):
    text = text.lower()
    words = re.findall(r'\b\w+\b', text)
    return words

def ngrams(words, n=2):
    return list(zip(*[words[i:] for i in range(n)]))

def train_ngram_model(corpus):
    model = defaultdict(Counter)
    for word1, word2 in ngrams(corpus):
        model[word1][word2] += 1
    return model

def get_unigram_probabilities(corpus):
    unigram_counts = Counter(corpus)
    total_count = len(corpus)
    return {word: count / total_count for word, count in unigram_counts.items()}

def perplexity(test_data, model, unigram_probs, alpha=1):
    total_log_prob = 0
    test_bigrams = ngrams(test_data)
    for word1, word2 in test_bigrams:
        prob = get_probability(model, word1, word2, unigram_probs, alpha)
        total_log_prob += -1 * log2(prob)
    return 2 ** (total_log_prob / len(test_bigrams))

def get_probability(model, word1, word2, unigram_probs, alpha):
    word1_count = sum(model[word1].values())
    word2_given_word1_count = model[word1][word2]
    if word1_count == 0:
        return alpha * unigram_probs.get(word2, 1e-10)
    prob = word2_given_word1_count / word1_count
    return prob + alpha * unigram_probs.get(word2, 1e-10)

datasets = {
    'wikitext': {
        'train': 'wiki.train.raw',
        'valid': 'wiki.valid.raw',
        'test': 'wiki.test.raw',
    },
    'ptb': {
        'train': 'ptb.train.txt',
        'valid': 'ptb.valid.txt',
        'test': 'ptb.test.txt',
    }
}

preprocessed_data = {}
for dataset_name, dataset_files in datasets.items():
    preprocessed_data[dataset_name] = {}
    for data_type, file_path in dataset_files.items():
        if file_path:
            with open(file_path, 'r') as f:
                text = f.read()
            preprocessed_data[dataset_name][data_type] = preprocess(text)

models = {}
unigram_probs = {}
for dataset_name, preprocessed_texts in preprocessed_data.items():
    # Train 2-gram model on the training dataset
    models[dataset_name] = train_ngram_model(preprocessed_texts['train'])
    unigram_probs[dataset_name] = get_unigram_probabilities(preprocessed_texts['train'])

for dataset_name, model in models.items():
    print(f'{dataset_name.capitalize()} dataset results (with back-off):')
    for data_type, preprocessed_text in preprocessed_data[dataset_name].items():
        perplexity_value = perplexity(preprocessed_text, model, unigram_probs[dataset_name])
        print(f'  {data_type.capitalize()} Perplexity:', perplexity_value)
    print()